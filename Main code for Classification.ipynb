{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 513)\n",
      "(9376, 513)\n",
      "(9376,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "### Import vibration signal of motor speed of 800 apm\n",
    "\n",
    "Case1 = pd.read_csv('Case1_800.csv')\n",
    "Case2 = pd.read_csv(\"Case2_800.csv\")\n",
    "Case3 = pd.read_csv(\"Case3_800.csv\")\n",
    "Case4 = pd.read_csv(\"Case4_800.csv\")\n",
    "Case5 = pd.read_csv(\"Case5_800.csv\")\n",
    "Case6 = pd.read_csv(\"Case6_800.csv\")\n",
    "Case7 = pd.read_csv(\"Case7_800.csv\")\n",
    "Case8 = pd.read_csv(\"Case8_800.csv\")\n",
    "Case9 = pd.read_csv(\"Case9_800.csv\")\n",
    "Case10 = pd.read_csv(\"Case10_800.csv\")\n",
    "Case11 = pd.read_csv(\"Case11_800.csv\")\n",
    "Case12 = pd.read_csv(\"Case12_800.csv\")\n",
    "Case13 = pd.read_csv(\"Case13_800.csv\")\n",
    "Case14 = pd.read_csv(\"Case14_800.csv\")\n",
    "Case15 = pd.read_csv(\"Case15_800.csv\")\n",
    "Case16 = pd.read_csv(\"Case16_800.csv\")\n",
    "\n",
    "### Getting the data for Sensor 1\n",
    "\n",
    "Case1 = Case1.iloc[:, 0].values\n",
    "Case2 = Case2.iloc[:, 0].values\n",
    "Case3 = Case3.iloc[:, 0].values\n",
    "Case4 = Case4.iloc[:, 0].values\n",
    "Case5 = Case5.iloc[:, 0].values\n",
    "Case6 = Case6.iloc[:, 0].values\n",
    "Case7 = Case7.iloc[:, 0].values\n",
    "Case8 = Case8.iloc[:, 0].values\n",
    "Case9 = Case9.iloc[:, 0].values\n",
    "Case10 = Case10.iloc[:, 0].values\n",
    "Case11 = Case11.iloc[:, 0].values\n",
    "Case12 = Case12.iloc[:, 0].values\n",
    "Case13 = Case13.iloc[:, 0].values\n",
    "Case14 = Case14.iloc[:, 0].values\n",
    "Case15 = Case15.iloc[:, 0].values\n",
    "Case16 = Case16.iloc[:, 0].values\n",
    "\n",
    "### Feature extraction of the data (stft)\n",
    "\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 1024 # window in num. of samples\n",
    "\n",
    "stft1 = librosa.stft(Case1[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft1 = np.abs(stft1).T\n",
    "stft2 = librosa.stft(Case2[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft2 = np.abs(stft2).T\n",
    "stft3 = librosa.stft(Case3[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft3 = np.abs(stft3).T\n",
    "stft4 = librosa.stft(Case4[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft4 = np.abs(stft4).T\n",
    "stft5 = librosa.stft(Case5[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft5 = np.abs(stft5).T\n",
    "stft6 = librosa.stft(Case6[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft6 = np.abs(stft6).T\n",
    "stft7 = librosa.stft(Case7[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft7 = np.abs(stft7).T\n",
    "stft8 = librosa.stft(Case8[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft8 = np.abs(stft8).T\n",
    "stft9 = librosa.stft(Case9[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft9 = np.abs(stft9).T\n",
    "stft10 = librosa.stft(Case10[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft10 = np.abs(stft10).T\n",
    "stft11 = librosa.stft(Case11[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft11 = np.abs(stft11).T\n",
    "stft12 = librosa.stft(Case12[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft12 = np.abs(stft12).T\n",
    "stft13 = librosa.stft(Case13[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft13 = np.abs(stft13).T\n",
    "stft14 = librosa.stft(Case14[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft14 = np.abs(stft14).T\n",
    "stft15 = librosa.stft(Case15[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft15 = np.abs(stft15).T\n",
    "stft16 = librosa.stft(Case16[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft16 = np.abs(stft16).T\n",
    "print(stft1.shape)\n",
    "\n",
    "### Labeling the data\n",
    "\n",
    "y1 = np.zeros(len(stft1))\n",
    "y2 = np.ones(len(stft2))\n",
    "y3 = 2*np.ones(len(stft3))\n",
    "y4 = 3*np.ones(len(stft4))\n",
    "y5 = 4*np.ones(len(stft5))\n",
    "y6 = 5*np.ones(len(stft6))\n",
    "y7 = 6*np.ones(len(stft7))\n",
    "y8 = 7*np.ones(len(stft8))\n",
    "y9 = 8*np.ones(len(stft9))\n",
    "y10 = 9*np.ones(len(stft10))\n",
    "y11 = 10*np.ones(len(stft11))\n",
    "y12 = 11*np.ones(len(stft12))\n",
    "y13 = 12*np.ones(len(stft13))\n",
    "y14 = 13*np.ones(len(stft14))\n",
    "y15 = 14*np.ones(len(stft15))\n",
    "y16 = 15*np.ones(len(stft16))\n",
    "\n",
    "### Gathering all features\n",
    "\n",
    "X1 = np.vstack((stft1, stft2))\n",
    "X2 = np.vstack((X1, stft3))\n",
    "X3 = np.vstack((X2, stft4))\n",
    "X4 = np.vstack((X3, stft5))\n",
    "X5 = np.vstack((X4, stft6))\n",
    "X6 = np.vstack((X5, stft7))\n",
    "X7 = np.vstack((X6, stft8))\n",
    "X8 = np.vstack((X7, stft9))\n",
    "X9 = np.vstack((X8, stft10))\n",
    "X10 = np.vstack((X9, stft11))\n",
    "X11 = np.vstack((X10, stft12))\n",
    "X12 = np.vstack((X11, stft13))\n",
    "X13 = np.vstack((X12, stft14))\n",
    "X14 = np.vstack((X13, stft15))\n",
    "X_feature_800 = np.vstack((X14, stft16))\n",
    "\n",
    "### Gathering all labels\n",
    "\n",
    "y_1 = np.hstack((y1, y2))\n",
    "y_2 = np.hstack((y_1, y3))\n",
    "y_3 = np.hstack((y_2, y4))\n",
    "y_4 = np.hstack((y_3, y5))\n",
    "y_5 = np.hstack((y_4, y6))\n",
    "y_6 = np.hstack((y_5, y7))\n",
    "y_7 = np.hstack((y_6, y8))\n",
    "y_8 = np.hstack((y_7, y9))\n",
    "y_9 = np.hstack((y_8, y10))\n",
    "y_10 = np.hstack((y_9, y11))\n",
    "y_11 = np.hstack((y_10, y12))\n",
    "y_12 = np.hstack((y_11, y13))\n",
    "y_13 = np.hstack((y_12, y14))\n",
    "y_14 = np.hstack((y_13, y15))\n",
    "y_labels_800 = np.hstack((y_14, y16))\n",
    "\n",
    "print(X_feature_800.shape)\n",
    "print(y_labels_800.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 513)\n",
      "(9376, 513)\n",
      "(9376,)\n"
     ]
    }
   ],
   "source": [
    "### Import vibration signal of motor speed of 1000 apm\n",
    "\n",
    "Case1 = pd.read_csv('Case1_1000.csv')\n",
    "Case2 = pd.read_csv(\"Case2_1000.csv\")\n",
    "Case3 = pd.read_csv(\"Case3_1000.csv\")\n",
    "Case4 = pd.read_csv(\"Case4_1000.csv\")\n",
    "Case5 = pd.read_csv(\"Case5_1000.csv\")\n",
    "Case6 = pd.read_csv(\"Case6_1000.csv\")\n",
    "Case7 = pd.read_csv(\"Case7_1000.csv\")\n",
    "Case8 = pd.read_csv(\"Case8_1000.csv\")\n",
    "Case9 = pd.read_csv(\"Case9_1000.csv\")\n",
    "Case10 = pd.read_csv(\"Case10_1000.csv\")\n",
    "Case11 = pd.read_csv(\"Case11_1000.csv\")\n",
    "Case12 = pd.read_csv(\"Case12_1000.csv\")\n",
    "Case13 = pd.read_csv(\"Case13_1000.csv\")\n",
    "Case14 = pd.read_csv(\"Case14_1000.csv\")\n",
    "Case15 = pd.read_csv(\"Case15_1000.csv\")\n",
    "Case16 = pd.read_csv(\"Case16_1000.csv\")\n",
    "\n",
    "### Getting the data for Sensor 1\n",
    "\n",
    "Case1 = Case1.iloc[:, 0].values\n",
    "Case2 = Case2.iloc[:, 0].values\n",
    "Case3 = Case3.iloc[:, 0].values\n",
    "Case4 = Case4.iloc[:, 0].values\n",
    "Case5 = Case5.iloc[:, 0].values\n",
    "Case6 = Case6.iloc[:, 0].values\n",
    "Case7 = Case7.iloc[:, 0].values\n",
    "Case8 = Case8.iloc[:, 0].values\n",
    "Case9 = Case9.iloc[:, 0].values\n",
    "Case10 = Case10.iloc[:, 0].values\n",
    "Case11 = Case11.iloc[:, 0].values\n",
    "Case12 = Case12.iloc[:, 0].values\n",
    "Case13 = Case13.iloc[:, 0].values\n",
    "Case14 = Case14.iloc[:, 0].values\n",
    "Case15 = Case15.iloc[:, 0].values\n",
    "Case16 = Case16.iloc[:, 0].values\n",
    "\n",
    "### Feature extraction of the data (stft)\n",
    "\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 1024 # window in num. of samples\n",
    "\n",
    "stft1 = librosa.stft(Case1[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft1 = np.abs(stft1).T\n",
    "stft2 = librosa.stft(Case2[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft2 = np.abs(stft2).T\n",
    "stft3 = librosa.stft(Case3[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft3 = np.abs(stft3).T\n",
    "stft4 = librosa.stft(Case4[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft4 = np.abs(stft4).T\n",
    "stft5 = librosa.stft(Case5[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft5 = np.abs(stft5).T\n",
    "stft6 = librosa.stft(Case6[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft6 = np.abs(stft6).T\n",
    "stft7 = librosa.stft(Case7[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft7 = np.abs(stft7).T\n",
    "stft8 = librosa.stft(Case8[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft8 = np.abs(stft8).T\n",
    "stft9 = librosa.stft(Case9[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft9 = np.abs(stft9).T\n",
    "stft10 = librosa.stft(Case10[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft10 = np.abs(stft10).T\n",
    "stft11 = librosa.stft(Case11[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft11 = np.abs(stft11).T\n",
    "stft12 = librosa.stft(Case12[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft12 = np.abs(stft12).T\n",
    "stft13 = librosa.stft(Case13[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft13 = np.abs(stft13).T\n",
    "stft14 = librosa.stft(Case14[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft14 = np.abs(stft14).T\n",
    "stft15 = librosa.stft(Case15[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft15 = np.abs(stft15).T\n",
    "stft16 = librosa.stft(Case16[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft16 = np.abs(stft16).T\n",
    "print(stft1.shape)\n",
    "\n",
    "### Labeling the data\n",
    "\n",
    "y1 = np.zeros(len(stft1))\n",
    "y2 = np.ones(len(stft2))\n",
    "y3 = 2*np.ones(len(stft3))\n",
    "y4 = 3*np.ones(len(stft4))\n",
    "y5 = 4*np.ones(len(stft5))\n",
    "y6 = 5*np.ones(len(stft6))\n",
    "y7 = 6*np.ones(len(stft7))\n",
    "y8 = 7*np.ones(len(stft8))\n",
    "y9 = 8*np.ones(len(stft9))\n",
    "y10 = 9*np.ones(len(stft10))\n",
    "y11 = 10*np.ones(len(stft11))\n",
    "y12 = 11*np.ones(len(stft12))\n",
    "y13 = 12*np.ones(len(stft13))\n",
    "y14 = 13*np.ones(len(stft14))\n",
    "y15 = 14*np.ones(len(stft15))\n",
    "y16 = 15*np.ones(len(stft16))\n",
    "\n",
    "### Gathering all features\n",
    "\n",
    "X1 = np.vstack((stft1, stft2))\n",
    "X2 = np.vstack((X1, stft3))\n",
    "X3 = np.vstack((X2, stft4))\n",
    "X4 = np.vstack((X3, stft5))\n",
    "X5 = np.vstack((X4, stft6))\n",
    "X6 = np.vstack((X5, stft7))\n",
    "X7 = np.vstack((X6, stft8))\n",
    "X8 = np.vstack((X7, stft9))\n",
    "X9 = np.vstack((X8, stft10))\n",
    "X10 = np.vstack((X9, stft11))\n",
    "X11 = np.vstack((X10, stft12))\n",
    "X12 = np.vstack((X11, stft13))\n",
    "X13 = np.vstack((X12, stft14))\n",
    "X14 = np.vstack((X13, stft15))\n",
    "X_feature_1000 = np.vstack((X14, stft16))\n",
    "\n",
    "### Gathering all labels\n",
    "\n",
    "y_1 = np.hstack((y1, y2))\n",
    "y_2 = np.hstack((y_1, y3))\n",
    "y_3 = np.hstack((y_2, y4))\n",
    "y_4 = np.hstack((y_3, y5))\n",
    "y_5 = np.hstack((y_4, y6))\n",
    "y_6 = np.hstack((y_5, y7))\n",
    "y_7 = np.hstack((y_6, y8))\n",
    "y_8 = np.hstack((y_7, y9))\n",
    "y_9 = np.hstack((y_8, y10))\n",
    "y_10 = np.hstack((y_9, y11))\n",
    "y_11 = np.hstack((y_10, y12))\n",
    "y_12 = np.hstack((y_11, y13))\n",
    "y_13 = np.hstack((y_12, y14))\n",
    "y_14 = np.hstack((y_13, y15))\n",
    "y_labels_1000 = np.hstack((y_14, y16))\n",
    "\n",
    "print(X_feature_1000.shape)\n",
    "print(y_labels_1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 513)\n",
      "(9376, 513)\n",
      "(9376,)\n"
     ]
    }
   ],
   "source": [
    "### Import vibration signal of motor speed of 1200 apm\n",
    "\n",
    "Case1 = pd.read_csv('Case1_1200.csv')\n",
    "Case2 = pd.read_csv(\"Case2_1200.csv\")\n",
    "Case3 = pd.read_csv(\"Case3_1200.csv\")\n",
    "Case4 = pd.read_csv(\"Case4_1200.csv\")\n",
    "Case5 = pd.read_csv(\"Case5_1200.csv\")\n",
    "Case6 = pd.read_csv(\"Case6_1200.csv\")\n",
    "Case7 = pd.read_csv(\"Case7_1200.csv\")\n",
    "Case8 = pd.read_csv(\"Case8_1200.csv\")\n",
    "Case9 = pd.read_csv(\"Case9_1200.csv\")\n",
    "Case10 = pd.read_csv(\"Case10_1200.csv\")\n",
    "Case11 = pd.read_csv(\"Case11_1200.csv\")\n",
    "Case12 = pd.read_csv(\"Case12_1200.csv\")\n",
    "Case13 = pd.read_csv(\"Case13_1200.csv\")\n",
    "Case14 = pd.read_csv(\"Case14_1200.csv\")\n",
    "Case15 = pd.read_csv(\"Case15_1200.csv\")\n",
    "Case16 = pd.read_csv(\"Case16_1200.csv\")\n",
    "\n",
    "### Getting the data for Sensor 1\n",
    "\n",
    "Case1 = Case1.iloc[:, 0].values\n",
    "Case2 = Case2.iloc[:, 0].values\n",
    "Case3 = Case3.iloc[:, 0].values\n",
    "Case4 = Case4.iloc[:, 0].values\n",
    "Case5 = Case5.iloc[:, 0].values\n",
    "Case6 = Case6.iloc[:, 0].values\n",
    "Case7 = Case7.iloc[:, 0].values\n",
    "Case8 = Case8.iloc[:, 0].values\n",
    "Case9 = Case9.iloc[:, 0].values\n",
    "Case10 = Case10.iloc[:, 0].values\n",
    "Case11 = Case11.iloc[:, 0].values\n",
    "Case12 = Case12.iloc[:, 0].values\n",
    "Case13 = Case13.iloc[:, 0].values\n",
    "Case14 = Case14.iloc[:, 0].values\n",
    "Case15 = Case15.iloc[:, 0].values\n",
    "Case16 = Case16.iloc[:, 0].values\n",
    "\n",
    "### Feature extraction of the data (stft)\n",
    "\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 1024 # window in num. of samples\n",
    "\n",
    "stft1 = librosa.stft(Case1[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft1 = np.abs(stft1).T\n",
    "stft2 = librosa.stft(Case2[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft2 = np.abs(stft2).T\n",
    "stft3 = librosa.stft(Case3[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft3 = np.abs(stft3).T\n",
    "stft4 = librosa.stft(Case4[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft4 = np.abs(stft4).T\n",
    "stft5 = librosa.stft(Case5[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft5 = np.abs(stft5).T\n",
    "stft6 = librosa.stft(Case6[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft6 = np.abs(stft6).T\n",
    "stft7 = librosa.stft(Case7[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft7 = np.abs(stft7).T\n",
    "stft8 = librosa.stft(Case8[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft8 = np.abs(stft8).T\n",
    "stft9 = librosa.stft(Case9[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft9 = np.abs(stft9).T\n",
    "stft10 = librosa.stft(Case10[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft10 = np.abs(stft10).T\n",
    "stft11 = librosa.stft(Case11[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft11 = np.abs(stft11).T\n",
    "stft12 = librosa.stft(Case12[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft12 = np.abs(stft12).T\n",
    "stft13 = librosa.stft(Case13[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft13 = np.abs(stft13).T\n",
    "stft14 = librosa.stft(Case14[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft14 = np.abs(stft14).T\n",
    "stft15 = librosa.stft(Case15[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft15 = np.abs(stft15).T\n",
    "stft16 = librosa.stft(Case16[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft16 = np.abs(stft16).T\n",
    "print(stft1.shape)\n",
    "\n",
    "### Labeling the data\n",
    "\n",
    "y1 = np.zeros(len(stft1))\n",
    "y2 = np.ones(len(stft2))\n",
    "y3 = 2*np.ones(len(stft3))\n",
    "y4 = 3*np.ones(len(stft4))\n",
    "y5 = 4*np.ones(len(stft5))\n",
    "y6 = 5*np.ones(len(stft6))\n",
    "y7 = 6*np.ones(len(stft7))\n",
    "y8 = 7*np.ones(len(stft8))\n",
    "y9 = 8*np.ones(len(stft9))\n",
    "y10 = 9*np.ones(len(stft10))\n",
    "y11 = 10*np.ones(len(stft11))\n",
    "y12 = 11*np.ones(len(stft12))\n",
    "y13 = 12*np.ones(len(stft13))\n",
    "y14 = 13*np.ones(len(stft14))\n",
    "y15 = 14*np.ones(len(stft15))\n",
    "y16 = 15*np.ones(len(stft16))\n",
    "\n",
    "### Gathering all features\n",
    "\n",
    "X1 = np.vstack((stft1, stft2))\n",
    "X2 = np.vstack((X1, stft3))\n",
    "X3 = np.vstack((X2, stft4))\n",
    "X4 = np.vstack((X3, stft5))\n",
    "X5 = np.vstack((X4, stft6))\n",
    "X6 = np.vstack((X5, stft7))\n",
    "X7 = np.vstack((X6, stft8))\n",
    "X8 = np.vstack((X7, stft9))\n",
    "X9 = np.vstack((X8, stft10))\n",
    "X10 = np.vstack((X9, stft11))\n",
    "X11 = np.vstack((X10, stft12))\n",
    "X12 = np.vstack((X11, stft13))\n",
    "X13 = np.vstack((X12, stft14))\n",
    "X14 = np.vstack((X13, stft15))\n",
    "X_feature_1200 = np.vstack((X14, stft16))\n",
    "\n",
    "### Gathering all labels\n",
    "\n",
    "y_1 = np.hstack((y1, y2))\n",
    "y_2 = np.hstack((y_1, y3))\n",
    "y_3 = np.hstack((y_2, y4))\n",
    "y_4 = np.hstack((y_3, y5))\n",
    "y_5 = np.hstack((y_4, y6))\n",
    "y_6 = np.hstack((y_5, y7))\n",
    "y_7 = np.hstack((y_6, y8))\n",
    "y_8 = np.hstack((y_7, y9))\n",
    "y_9 = np.hstack((y_8, y10))\n",
    "y_10 = np.hstack((y_9, y11))\n",
    "y_11 = np.hstack((y_10, y12))\n",
    "y_12 = np.hstack((y_11, y13))\n",
    "y_13 = np.hstack((y_12, y14))\n",
    "y_14 = np.hstack((y_13, y15))\n",
    "y_labels_1200 = np.hstack((y_14, y16))\n",
    "\n",
    "print(X_feature_1200.shape)\n",
    "print(y_labels_1200.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28128, 513)\n",
      "(28128,)\n",
      "(22502, 513)\n",
      "(5626, 513)\n",
      "(22502,)\n",
      "(5626,)\n"
     ]
    }
   ],
   "source": [
    "### Adding features of all operating condition\n",
    "\n",
    "X01 = np.vstack((X_feature_800, X_feature_1000))\n",
    "X_sensor1 = np.vstack((X01, X_feature_1200))\n",
    "\n",
    "y_01 = np.hstack((y_labels_800, y_labels_1000))\n",
    "y_sensor1 = np.hstack((y_01, y_labels_1200))\n",
    "\n",
    "print(X_sensor1.shape)\n",
    "print(y_sensor1.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sensor1, y_sensor1, test_size = 0.2, random_state=0)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time of RF : 67.43135070800781s\n",
      "The accuracy of RandomForest:  75.34660504799146 %\n",
      "Training time of BT : 174.04733538627625s\n",
      "The accuracy of BaggingClassifier:  74.03128332740846 %\n",
      "Training time of DT : 29.4467933177948s\n",
      "The accuracy of Decision Tree:  63.09989335229292 %\n",
      "Training time of KNN : 3.5845768451690674s\n",
      "The accuracy of KNeighborsClassifier:  59.580519018841095 %\n",
      "Training time of LDA : 1.647813320159912s\n",
      "The accuracy of LDA Classifier:  51.8485602559545 %\n",
      "Training time of SVM : 295.37200570106506s\n",
      "The accuracy of SVM:  61.41130465694987 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oybek\\.conda\\envs\\tf_GPU\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:48:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training time of XGB : 611.6096575260162s\n",
      "The Accuracy of XGBClassifier:  87.94880910060434 %\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "RFC = RandomForestClassifier()\n",
    "start1 = time.time()\n",
    "RFC.fit(X_train,y_train)\n",
    "stop1 = time.time()\n",
    "print(f'Training time of RF : {stop1-start1}s')\n",
    "res_RFC = RFC.score(X_test, y_test)\n",
    "print(\"The accuracy of RandomForest: \", res_RFC*100, \"%\")\n",
    "\n",
    "## BaggingClassifier\n",
    "\n",
    "Bag = BaggingClassifier()\n",
    "start5 = time.time()\n",
    "Bag.fit(X_train,y_train)\n",
    "stop5 = time.time()\n",
    "print(f'Training time of BT : {stop5-start5}s')\n",
    "res_Bag = Bag.score(X_test, y_test)\n",
    "print(\"The accuracy of BaggingClassifier: \", res_Bag*100, \"%\")\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "start3 = time.time()\n",
    "DTC.fit(X_train,y_train)\n",
    "stop3 = time.time()\n",
    "print(f'Training time of DT : {stop3-start3}s')\n",
    "res_DTC = DTC.score(X_test, y_test)\n",
    "print(\"The accuracy of Decision Tree: \", res_DTC*100, \"%\")\n",
    "\n",
    "## KNeighborsClassifier\n",
    "\n",
    "Knn = KNeighborsClassifier()\n",
    "start4 = time.time()\n",
    "Knn.fit(X_train,y_train)\n",
    "stop4 = time.time()\n",
    "print(f'Training time of KNN : {stop4-start4}s')\n",
    "res_Knn = Knn.score(X_test, y_test)\n",
    "print(\"The accuracy of KNeighborsClassifier: \", res_Knn*100, \"%\")\n",
    "\n",
    "## LinearDiscriminantAnalysis\n",
    "dum = LinearDiscriminantAnalysis()\n",
    "start8 = time.time()\n",
    "dum.fit(X_train,y_train)\n",
    "stop8 = time.time()\n",
    "print(f'Training time of LDA : {stop8-start8}s')\n",
    "res_dum = dum.score(X_test, y_test)\n",
    "print(\"The accuracy of LDA Classifier: \", res_dum*100, \"%\")\n",
    "\n",
    "## SVM\n",
    "\n",
    "svm = SVC()\n",
    "start6 = time.time()\n",
    "svm.fit(X_train,y_train)\n",
    "stop6 = time.time()\n",
    "print(f'Training time of SVM : {stop6-start6}s')\n",
    "res_svm = svm.score(X_test, y_test)\n",
    "print(\"The accuracy of SVM: \", res_svm*100, \"%\")\n",
    "\n",
    "## XGBClassifier\n",
    "\n",
    "mod_XCB = xgboost.XGBClassifier()\n",
    "start7 = time.time()\n",
    "mod_XCB.fit(X_train,y_train)\n",
    "stop7 = time.time()\n",
    "print(f'Training time of XGB : {stop7-start7}s')\n",
    "result8 = mod_XCB.score(X_test, y_test)\n",
    "print('The Accuracy of XGBClassifier: ', result8*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 513)\n",
      "(9376, 513)\n",
      "(9376,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "### Import vibration signal of motor speed of 800 apm\n",
    "\n",
    "Case1 = pd.read_csv('Case1_800.csv')\n",
    "Case2 = pd.read_csv(\"Case2_800.csv\")\n",
    "Case3 = pd.read_csv(\"Case3_800.csv\")\n",
    "Case4 = pd.read_csv(\"Case4_800.csv\")\n",
    "Case5 = pd.read_csv(\"Case5_800.csv\")\n",
    "Case6 = pd.read_csv(\"Case6_800.csv\")\n",
    "Case7 = pd.read_csv(\"Case7_800.csv\")\n",
    "Case8 = pd.read_csv(\"Case8_800.csv\")\n",
    "Case9 = pd.read_csv(\"Case9_800.csv\")\n",
    "Case10 = pd.read_csv(\"Case10_800.csv\")\n",
    "Case11 = pd.read_csv(\"Case11_800.csv\")\n",
    "Case12 = pd.read_csv(\"Case12_800.csv\")\n",
    "Case13 = pd.read_csv(\"Case13_800.csv\")\n",
    "Case14 = pd.read_csv(\"Case14_800.csv\")\n",
    "Case15 = pd.read_csv(\"Case15_800.csv\")\n",
    "Case16 = pd.read_csv(\"Case16_800.csv\")\n",
    "\n",
    "### Getting the data for Sensor 2\n",
    "\n",
    "Case1 = Case1.iloc[:, 1].values\n",
    "Case2 = Case2.iloc[:, 1].values\n",
    "Case3 = Case3.iloc[:, 1].values\n",
    "Case4 = Case4.iloc[:, 1].values\n",
    "Case5 = Case5.iloc[:, 1].values\n",
    "Case6 = Case6.iloc[:, 1].values\n",
    "Case7 = Case7.iloc[:, 1].values\n",
    "Case8 = Case8.iloc[:, 1].values\n",
    "Case9 = Case9.iloc[:, 1].values\n",
    "Case10 = Case10.iloc[:, 1].values\n",
    "Case11 = Case11.iloc[:, 1].values\n",
    "Case12 = Case12.iloc[:, 1].values\n",
    "Case13 = Case13.iloc[:, 1].values\n",
    "Case14 = Case14.iloc[:, 1].values\n",
    "Case15 = Case15.iloc[:, 1].values\n",
    "Case16 = Case16.iloc[:, 1].values\n",
    "\n",
    "### Feature extraction of the data (stft)\n",
    "\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 1024 # window in num. of samples\n",
    "\n",
    "stft1 = librosa.stft(Case1[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft1 = np.abs(stft1).T\n",
    "stft2 = librosa.stft(Case2[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft2 = np.abs(stft2).T\n",
    "stft3 = librosa.stft(Case3[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft3 = np.abs(stft3).T\n",
    "stft4 = librosa.stft(Case4[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft4 = np.abs(stft4).T\n",
    "stft5 = librosa.stft(Case5[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft5 = np.abs(stft5).T\n",
    "stft6 = librosa.stft(Case6[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft6 = np.abs(stft6).T\n",
    "stft7 = librosa.stft(Case7[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft7 = np.abs(stft7).T\n",
    "stft8 = librosa.stft(Case8[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft8 = np.abs(stft8).T\n",
    "stft9 = librosa.stft(Case9[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft9 = np.abs(stft9).T\n",
    "stft10 = librosa.stft(Case10[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft10 = np.abs(stft10).T\n",
    "stft11 = librosa.stft(Case11[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft11 = np.abs(stft11).T\n",
    "stft12 = librosa.stft(Case12[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft12 = np.abs(stft12).T\n",
    "stft13 = librosa.stft(Case13[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft13 = np.abs(stft13).T\n",
    "stft14 = librosa.stft(Case14[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft14 = np.abs(stft14).T\n",
    "stft15 = librosa.stft(Case15[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft15 = np.abs(stft15).T\n",
    "stft16 = librosa.stft(Case16[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft16 = np.abs(stft16).T\n",
    "print(stft1.shape)\n",
    "\n",
    "### Labeling the data\n",
    "\n",
    "y1 = np.zeros(len(stft1))\n",
    "y2 = np.ones(len(stft2))\n",
    "y3 = 2*np.ones(len(stft3))\n",
    "y4 = 3*np.ones(len(stft4))\n",
    "y5 = 4*np.ones(len(stft5))\n",
    "y6 = 5*np.ones(len(stft6))\n",
    "y7 = 6*np.ones(len(stft7))\n",
    "y8 = 7*np.ones(len(stft8))\n",
    "y9 = 8*np.ones(len(stft9))\n",
    "y10 = 9*np.ones(len(stft10))\n",
    "y11 = 10*np.ones(len(stft11))\n",
    "y12 = 11*np.ones(len(stft12))\n",
    "y13 = 12*np.ones(len(stft13))\n",
    "y14 = 13*np.ones(len(stft14))\n",
    "y15 = 14*np.ones(len(stft15))\n",
    "y16 = 15*np.ones(len(stft16))\n",
    "\n",
    "### Gathering all features\n",
    "\n",
    "X1 = np.vstack((stft1, stft2))\n",
    "X2 = np.vstack((X1, stft3))\n",
    "X3 = np.vstack((X2, stft4))\n",
    "X4 = np.vstack((X3, stft5))\n",
    "X5 = np.vstack((X4, stft6))\n",
    "X6 = np.vstack((X5, stft7))\n",
    "X7 = np.vstack((X6, stft8))\n",
    "X8 = np.vstack((X7, stft9))\n",
    "X9 = np.vstack((X8, stft10))\n",
    "X10 = np.vstack((X9, stft11))\n",
    "X11 = np.vstack((X10, stft12))\n",
    "X12 = np.vstack((X11, stft13))\n",
    "X13 = np.vstack((X12, stft14))\n",
    "X14 = np.vstack((X13, stft15))\n",
    "X_feature_800 = np.vstack((X14, stft16))\n",
    "\n",
    "### Gathering all labels\n",
    "\n",
    "y_1 = np.hstack((y1, y2))\n",
    "y_2 = np.hstack((y_1, y3))\n",
    "y_3 = np.hstack((y_2, y4))\n",
    "y_4 = np.hstack((y_3, y5))\n",
    "y_5 = np.hstack((y_4, y6))\n",
    "y_6 = np.hstack((y_5, y7))\n",
    "y_7 = np.hstack((y_6, y8))\n",
    "y_8 = np.hstack((y_7, y9))\n",
    "y_9 = np.hstack((y_8, y10))\n",
    "y_10 = np.hstack((y_9, y11))\n",
    "y_11 = np.hstack((y_10, y12))\n",
    "y_12 = np.hstack((y_11, y13))\n",
    "y_13 = np.hstack((y_12, y14))\n",
    "y_14 = np.hstack((y_13, y15))\n",
    "y_labels_800 = np.hstack((y_14, y16))\n",
    "\n",
    "print(X_feature_800.shape)\n",
    "print(y_labels_800.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 513)\n",
      "(9376, 513)\n",
      "(9376,)\n"
     ]
    }
   ],
   "source": [
    "### Import vibration signal of motor speed of 1000 apm\n",
    "\n",
    "Case1 = pd.read_csv('Case1_1000.csv')\n",
    "Case2 = pd.read_csv(\"Case2_1000.csv\")\n",
    "Case3 = pd.read_csv(\"Case3_1000.csv\")\n",
    "Case4 = pd.read_csv(\"Case4_1000.csv\")\n",
    "Case5 = pd.read_csv(\"Case5_1000.csv\")\n",
    "Case6 = pd.read_csv(\"Case6_1000.csv\")\n",
    "Case7 = pd.read_csv(\"Case7_1000.csv\")\n",
    "Case8 = pd.read_csv(\"Case8_1000.csv\")\n",
    "Case9 = pd.read_csv(\"Case9_1000.csv\")\n",
    "Case10 = pd.read_csv(\"Case10_1000.csv\")\n",
    "Case11 = pd.read_csv(\"Case11_1000.csv\")\n",
    "Case12 = pd.read_csv(\"Case12_1000.csv\")\n",
    "Case13 = pd.read_csv(\"Case13_1000.csv\")\n",
    "Case14 = pd.read_csv(\"Case14_1000.csv\")\n",
    "Case15 = pd.read_csv(\"Case15_1000.csv\")\n",
    "Case16 = pd.read_csv(\"Case16_1000.csv\")\n",
    "\n",
    "### Getting the data for Sensor 2\n",
    "\n",
    "Case1 = Case1.iloc[:, 1].values\n",
    "Case2 = Case2.iloc[:, 1].values\n",
    "Case3 = Case3.iloc[:, 1].values\n",
    "Case4 = Case4.iloc[:, 1].values\n",
    "Case5 = Case5.iloc[:, 1].values\n",
    "Case6 = Case6.iloc[:, 1].values\n",
    "Case7 = Case7.iloc[:, 1].values\n",
    "Case8 = Case8.iloc[:, 1].values\n",
    "Case9 = Case9.iloc[:, 1].values\n",
    "Case10 = Case10.iloc[:, 1].values\n",
    "Case11 = Case11.iloc[:, 1].values\n",
    "Case12 = Case12.iloc[:, 1].values\n",
    "Case13 = Case13.iloc[:, 1].values\n",
    "Case14 = Case14.iloc[:, 1].values\n",
    "Case15 = Case15.iloc[:, 1].values\n",
    "Case16 = Case16.iloc[:, 1].values\n",
    "\n",
    "### Feature extraction of the data (stft)\n",
    "\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 1024 # window in num. of samples\n",
    "\n",
    "stft1 = librosa.stft(Case1[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft1 = np.abs(stft1).T\n",
    "stft2 = librosa.stft(Case2[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft2 = np.abs(stft2).T\n",
    "stft3 = librosa.stft(Case3[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft3 = np.abs(stft3).T\n",
    "stft4 = librosa.stft(Case4[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft4 = np.abs(stft4).T\n",
    "stft5 = librosa.stft(Case5[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft5 = np.abs(stft5).T\n",
    "stft6 = librosa.stft(Case6[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft6 = np.abs(stft6).T\n",
    "stft7 = librosa.stft(Case7[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft7 = np.abs(stft7).T\n",
    "stft8 = librosa.stft(Case8[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft8 = np.abs(stft8).T\n",
    "stft9 = librosa.stft(Case9[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft9 = np.abs(stft9).T\n",
    "stft10 = librosa.stft(Case10[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft10 = np.abs(stft10).T\n",
    "stft11 = librosa.stft(Case11[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft11 = np.abs(stft11).T\n",
    "stft12 = librosa.stft(Case12[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft12 = np.abs(stft12).T\n",
    "stft13 = librosa.stft(Case13[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft13 = np.abs(stft13).T\n",
    "stft14 = librosa.stft(Case14[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft14 = np.abs(stft14).T\n",
    "stft15 = librosa.stft(Case15[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft15 = np.abs(stft15).T\n",
    "stft16 = librosa.stft(Case16[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft16 = np.abs(stft16).T\n",
    "print(stft1.shape)\n",
    "\n",
    "### Labeling the data\n",
    "\n",
    "y1 = np.zeros(len(stft1))\n",
    "y2 = np.ones(len(stft2))\n",
    "y3 = 2*np.ones(len(stft3))\n",
    "y4 = 3*np.ones(len(stft4))\n",
    "y5 = 4*np.ones(len(stft5))\n",
    "y6 = 5*np.ones(len(stft6))\n",
    "y7 = 6*np.ones(len(stft7))\n",
    "y8 = 7*np.ones(len(stft8))\n",
    "y9 = 8*np.ones(len(stft9))\n",
    "y10 = 9*np.ones(len(stft10))\n",
    "y11 = 10*np.ones(len(stft11))\n",
    "y12 = 11*np.ones(len(stft12))\n",
    "y13 = 12*np.ones(len(stft13))\n",
    "y14 = 13*np.ones(len(stft14))\n",
    "y15 = 14*np.ones(len(stft15))\n",
    "y16 = 15*np.ones(len(stft16))\n",
    "\n",
    "### Gathering all features\n",
    "\n",
    "X1 = np.vstack((stft1, stft2))\n",
    "X2 = np.vstack((X1, stft3))\n",
    "X3 = np.vstack((X2, stft4))\n",
    "X4 = np.vstack((X3, stft5))\n",
    "X5 = np.vstack((X4, stft6))\n",
    "X6 = np.vstack((X5, stft7))\n",
    "X7 = np.vstack((X6, stft8))\n",
    "X8 = np.vstack((X7, stft9))\n",
    "X9 = np.vstack((X8, stft10))\n",
    "X10 = np.vstack((X9, stft11))\n",
    "X11 = np.vstack((X10, stft12))\n",
    "X12 = np.vstack((X11, stft13))\n",
    "X13 = np.vstack((X12, stft14))\n",
    "X14 = np.vstack((X13, stft15))\n",
    "X_feature_1000 = np.vstack((X14, stft16))\n",
    "\n",
    "### Gathering all labels\n",
    "\n",
    "y_1 = np.hstack((y1, y2))\n",
    "y_2 = np.hstack((y_1, y3))\n",
    "y_3 = np.hstack((y_2, y4))\n",
    "y_4 = np.hstack((y_3, y5))\n",
    "y_5 = np.hstack((y_4, y6))\n",
    "y_6 = np.hstack((y_5, y7))\n",
    "y_7 = np.hstack((y_6, y8))\n",
    "y_8 = np.hstack((y_7, y9))\n",
    "y_9 = np.hstack((y_8, y10))\n",
    "y_10 = np.hstack((y_9, y11))\n",
    "y_11 = np.hstack((y_10, y12))\n",
    "y_12 = np.hstack((y_11, y13))\n",
    "y_13 = np.hstack((y_12, y14))\n",
    "y_14 = np.hstack((y_13, y15))\n",
    "y_labels_1000 = np.hstack((y_14, y16))\n",
    "\n",
    "print(X_feature_1000.shape)\n",
    "print(y_labels_1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 513)\n",
      "(9376, 513)\n",
      "(9376,)\n"
     ]
    }
   ],
   "source": [
    "### Import vibration signal of motor speed of 1200 apm\n",
    "\n",
    "Case1 = pd.read_csv('Case1_1200.csv')\n",
    "Case2 = pd.read_csv(\"Case2_1200.csv\")\n",
    "Case3 = pd.read_csv(\"Case3_1200.csv\")\n",
    "Case4 = pd.read_csv(\"Case4_1200.csv\")\n",
    "Case5 = pd.read_csv(\"Case5_1200.csv\")\n",
    "Case6 = pd.read_csv(\"Case6_1200.csv\")\n",
    "Case7 = pd.read_csv(\"Case7_1200.csv\")\n",
    "Case8 = pd.read_csv(\"Case8_1200.csv\")\n",
    "Case9 = pd.read_csv(\"Case9_1200.csv\")\n",
    "Case10 = pd.read_csv(\"Case10_1200.csv\")\n",
    "Case11 = pd.read_csv(\"Case11_1200.csv\")\n",
    "Case12 = pd.read_csv(\"Case12_1200.csv\")\n",
    "Case13 = pd.read_csv(\"Case13_1200.csv\")\n",
    "Case14 = pd.read_csv(\"Case14_1200.csv\")\n",
    "Case15 = pd.read_csv(\"Case15_1200.csv\")\n",
    "Case16 = pd.read_csv(\"Case16_1200.csv\")\n",
    "\n",
    "### Getting the data for Sensor 2\n",
    "\n",
    "Case1 = Case1.iloc[:, 1].values\n",
    "Case2 = Case2.iloc[:, 1].values\n",
    "Case3 = Case3.iloc[:, 1].values\n",
    "Case4 = Case4.iloc[:, 1].values\n",
    "Case5 = Case5.iloc[:, 1].values\n",
    "Case6 = Case6.iloc[:, 1].values\n",
    "Case7 = Case7.iloc[:, 1].values\n",
    "Case8 = Case8.iloc[:, 1].values\n",
    "Case9 = Case9.iloc[:, 1].values\n",
    "Case10 = Case10.iloc[:, 1].values\n",
    "Case11 = Case11.iloc[:, 1].values\n",
    "Case12 = Case12.iloc[:, 1].values\n",
    "Case13 = Case13.iloc[:, 1].values\n",
    "Case14 = Case14.iloc[:, 1].values\n",
    "Case15 = Case15.iloc[:, 1].values\n",
    "Case16 = Case16.iloc[:, 1].values\n",
    "\n",
    "### Feature extraction of the data (stft)\n",
    "\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 1024 # window in num. of samples\n",
    "\n",
    "stft1 = librosa.stft(Case1[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft1 = np.abs(stft1).T\n",
    "stft2 = librosa.stft(Case2[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft2 = np.abs(stft2).T\n",
    "stft3 = librosa.stft(Case3[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft3 = np.abs(stft3).T\n",
    "stft4 = librosa.stft(Case4[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft4 = np.abs(stft4).T\n",
    "stft5 = librosa.stft(Case5[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft5 = np.abs(stft5).T\n",
    "stft6 = librosa.stft(Case6[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft6 = np.abs(stft6).T\n",
    "stft7 = librosa.stft(Case7[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft7 = np.abs(stft7).T\n",
    "stft8 = librosa.stft(Case8[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft8 = np.abs(stft8).T\n",
    "stft9 = librosa.stft(Case9[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft9 = np.abs(stft9).T\n",
    "stft10 = librosa.stft(Case10[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft10 = np.abs(stft10).T\n",
    "stft11 = librosa.stft(Case11[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft11 = np.abs(stft11).T\n",
    "stft12 = librosa.stft(Case12[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft12 = np.abs(stft12).T\n",
    "stft13 = librosa.stft(Case13[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft13 = np.abs(stft13).T\n",
    "stft14 = librosa.stft(Case14[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft14 = np.abs(stft14).T\n",
    "stft15 = librosa.stft(Case15[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft15 = np.abs(stft15).T\n",
    "stft16 = librosa.stft(Case16[0:300000], n_fft=n_fft, hop_length=hop_length)\n",
    "stft16 = np.abs(stft16).T\n",
    "print(stft1.shape)\n",
    "\n",
    "### Labeling the data\n",
    "\n",
    "y1 = np.zeros(len(stft1))\n",
    "y2 = np.ones(len(stft2))\n",
    "y3 = 2*np.ones(len(stft3))\n",
    "y4 = 3*np.ones(len(stft4))\n",
    "y5 = 4*np.ones(len(stft5))\n",
    "y6 = 5*np.ones(len(stft6))\n",
    "y7 = 6*np.ones(len(stft7))\n",
    "y8 = 7*np.ones(len(stft8))\n",
    "y9 = 8*np.ones(len(stft9))\n",
    "y10 = 9*np.ones(len(stft10))\n",
    "y11 = 10*np.ones(len(stft11))\n",
    "y12 = 11*np.ones(len(stft12))\n",
    "y13 = 12*np.ones(len(stft13))\n",
    "y14 = 13*np.ones(len(stft14))\n",
    "y15 = 14*np.ones(len(stft15))\n",
    "y16 = 15*np.ones(len(stft16))\n",
    "\n",
    "### Gathering all features\n",
    "\n",
    "X1 = np.vstack((stft1, stft2))\n",
    "X2 = np.vstack((X1, stft3))\n",
    "X3 = np.vstack((X2, stft4))\n",
    "X4 = np.vstack((X3, stft5))\n",
    "X5 = np.vstack((X4, stft6))\n",
    "X6 = np.vstack((X5, stft7))\n",
    "X7 = np.vstack((X6, stft8))\n",
    "X8 = np.vstack((X7, stft9))\n",
    "X9 = np.vstack((X8, stft10))\n",
    "X10 = np.vstack((X9, stft11))\n",
    "X11 = np.vstack((X10, stft12))\n",
    "X12 = np.vstack((X11, stft13))\n",
    "X13 = np.vstack((X12, stft14))\n",
    "X14 = np.vstack((X13, stft15))\n",
    "X_feature_1200 = np.vstack((X14, stft16))\n",
    "\n",
    "### Gathering all labels\n",
    "\n",
    "y_1 = np.hstack((y1, y2))\n",
    "y_2 = np.hstack((y_1, y3))\n",
    "y_3 = np.hstack((y_2, y4))\n",
    "y_4 = np.hstack((y_3, y5))\n",
    "y_5 = np.hstack((y_4, y6))\n",
    "y_6 = np.hstack((y_5, y7))\n",
    "y_7 = np.hstack((y_6, y8))\n",
    "y_8 = np.hstack((y_7, y9))\n",
    "y_9 = np.hstack((y_8, y10))\n",
    "y_10 = np.hstack((y_9, y11))\n",
    "y_11 = np.hstack((y_10, y12))\n",
    "y_12 = np.hstack((y_11, y13))\n",
    "y_13 = np.hstack((y_12, y14))\n",
    "y_14 = np.hstack((y_13, y15))\n",
    "y_labels_1200 = np.hstack((y_14, y16))\n",
    "\n",
    "print(X_feature_1200.shape)\n",
    "print(y_labels_1200.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28128, 513)\n",
      "(28128,)\n",
      "(22502, 513)\n",
      "(5626, 513)\n",
      "(22502,)\n",
      "(5626,)\n"
     ]
    }
   ],
   "source": [
    "### Adding features of all operating condition\n",
    "\n",
    "X_01 = np.vstack((X_feature_800, X_feature_1000))\n",
    "X_sensor2 = np.vstack((X_01, X_feature_1200))\n",
    "\n",
    "y_01 = np.hstack((y_labels_800, y_labels_1000))\n",
    "y_sensor2 = np.hstack((y_01, y_labels_1200))\n",
    "\n",
    "print(X_sensor2.shape)\n",
    "print(y_sensor2.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sensor2, y_sensor2, test_size = 0.2, random_state=0)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time of RF : 64.32900619506836s\n",
      "The accuracy of RandomForest:  95.11198009242801 %\n",
      "Training time of BT : 139.48443007469177s\n",
      "The accuracy of BaggingClassifier:  92.6946320654106 %\n",
      "Training time of DT : 22.73936939239502s\n",
      "The accuracy of Decision Tree:  85.35371489512976 %\n",
      "Training time of KNN : 3.6325480937957764s\n",
      "The accuracy of KNeighborsClassifier:  64.50408816210451 %\n",
      "Training time of LDA : 1.7061200141906738s\n",
      "The accuracy of LDA Classifier:  65.51724137931035 %\n",
      "Training time of SVM : 427.317999124527s\n",
      "The accuracy of SVM:  33.78954852470672 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oybek\\.conda\\envs\\tf_GPU\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:10:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training time of XGB : 482.25302028656006s\n",
      "The Accuracy of XGBClassifier:  98.0447920369712 %\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "RFC = RandomForestClassifier()\n",
    "start1 = time.time()\n",
    "RFC.fit(X_train,y_train)\n",
    "stop1 = time.time()\n",
    "print(f'Training time of RF : {stop1-start1}s')\n",
    "res_RFC = RFC.score(X_test, y_test)\n",
    "print(\"The accuracy of RandomForest: \", res_RFC*100, \"%\")\n",
    "\n",
    "## BaggingClassifier\n",
    "\n",
    "Bag = BaggingClassifier()\n",
    "start5 = time.time()\n",
    "Bag.fit(X_train,y_train)\n",
    "stop5 = time.time()\n",
    "print(f'Training time of BT : {stop5-start5}s')\n",
    "res_Bag = Bag.score(X_test, y_test)\n",
    "print(\"The accuracy of BaggingClassifier: \", res_Bag*100, \"%\")\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "start3 = time.time()\n",
    "DTC.fit(X_train,y_train)\n",
    "stop3 = time.time()\n",
    "print(f'Training time of DT : {stop3-start3}s')\n",
    "res_DTC = DTC.score(X_test, y_test)\n",
    "print(\"The accuracy of Decision Tree: \", res_DTC*100, \"%\")\n",
    "\n",
    "## KNeighborsClassifier\n",
    "\n",
    "Knn = KNeighborsClassifier()\n",
    "start4 = time.time()\n",
    "Knn.fit(X_train,y_train)\n",
    "stop4 = time.time()\n",
    "print(f'Training time of KNN : {stop4-start4}s')\n",
    "res_Knn = Knn.score(X_test, y_test)\n",
    "print(\"The accuracy of KNeighborsClassifier: \", res_Knn*100, \"%\")\n",
    "\n",
    "## LinearDiscriminantAnalysis\n",
    "dum = LinearDiscriminantAnalysis()\n",
    "start8 = time.time()\n",
    "dum.fit(X_train,y_train)\n",
    "stop8 = time.time()\n",
    "print(f'Training time of LDA : {stop8-start8}s')\n",
    "res_dum = dum.score(X_test, y_test)\n",
    "print(\"The accuracy of LDA Classifier: \", res_dum*100, \"%\")\n",
    "\n",
    "## SVM\n",
    "\n",
    "svm = SVC()\n",
    "start6 = time.time()\n",
    "svm.fit(X_train,y_train)\n",
    "stop6 = time.time()\n",
    "print(f'Training time of SVM : {stop6-start6}s')\n",
    "res_svm = svm.score(X_test, y_test)\n",
    "print(\"The accuracy of SVM: \", res_svm*100, \"%\")\n",
    "\n",
    "## XGBClassifier\n",
    "\n",
    "mod_XCB = xgboost.XGBClassifier()\n",
    "start7 = time.time()\n",
    "mod_XCB.fit(X_train,y_train)\n",
    "stop7 = time.time()\n",
    "print(f'Training time of XGB : {stop7-start7}s')\n",
    "result8 = mod_XCB.score(X_test, y_test)\n",
    "print('The Accuracy of XGBClassifier: ', result8*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22502, 25)\n",
      "(5626, 25)\n",
      "(22502,)\n",
      "(5626,)\n"
     ]
    }
   ],
   "source": [
    "### Feature reduction\n",
    "### Picking up frequencies from 0 to 25 of sensor2 data\n",
    "\n",
    "X_reduced = X_sensor2[:, 0:25]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_sensor2, test_size = 0.2, random_state=0)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time of RF : 12.852901697158813s\n",
      "The accuracy of RandomForest:  95.89406327763953 %\n",
      "Training time of BT : 5.9950902462005615s\n",
      "The accuracy of BaggingClassifier:  92.74795591894774 %\n",
      "Training time of DT : 0.9347014427185059s\n",
      "The accuracy of Decision Tree:  87.89548524706719 %\n",
      "Training time of KNN : 0.14794921875s\n",
      "The accuracy of KNeighborsClassifier:  90.79274795591895 %\n",
      "Training time of LDA : 0.07197809219360352s\n",
      "The accuracy of LDA Classifier:  32.029861357980806 %\n",
      "Training time of SVM : 57.21979999542236s\n",
      "The accuracy of SVM:  6.6477070742979025 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oybek\\.conda\\envs\\tf_GPU\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:20:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training time of XGB : 42.298513412475586s\n",
      "The Accuracy of XGBClassifier:  96.16068254532527 %\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "RFC = RandomForestClassifier()\n",
    "start1 = time.time()\n",
    "RFC.fit(X_train,y_train)\n",
    "stop1 = time.time()\n",
    "print(f'Training time of RF : {stop1-start1}s')\n",
    "res_RFC = RFC.score(X_test, y_test)\n",
    "print(\"The accuracy of RandomForest: \", res_RFC*100, \"%\")\n",
    "\n",
    "## BaggingClassifier\n",
    "\n",
    "Bag = BaggingClassifier()\n",
    "start5 = time.time()\n",
    "Bag.fit(X_train,y_train)\n",
    "stop5 = time.time()\n",
    "print(f'Training time of BT : {stop5-start5}s')\n",
    "res_Bag = Bag.score(X_test, y_test)\n",
    "print(\"The accuracy of BaggingClassifier: \", res_Bag*100, \"%\")\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "start3 = time.time()\n",
    "DTC.fit(X_train,y_train)\n",
    "stop3 = time.time()\n",
    "print(f'Training time of DT : {stop3-start3}s')\n",
    "res_DTC = DTC.score(X_test, y_test)\n",
    "print(\"The accuracy of Decision Tree: \", res_DTC*100, \"%\")\n",
    "\n",
    "## KNeighborsClassifier\n",
    "\n",
    "Knn = KNeighborsClassifier()\n",
    "start4 = time.time()\n",
    "Knn.fit(X_train,y_train)\n",
    "stop4 = time.time()\n",
    "print(f'Training time of KNN : {stop4-start4}s')\n",
    "res_Knn = Knn.score(X_test, y_test)\n",
    "print(\"The accuracy of KNeighborsClassifier: \", res_Knn*100, \"%\")\n",
    "\n",
    "## LinearDiscriminantAnalysis\n",
    "dum = LinearDiscriminantAnalysis()\n",
    "start8 = time.time()\n",
    "dum.fit(X_train,y_train)\n",
    "stop8 = time.time()\n",
    "print(f'Training time of LDA : {stop8-start8}s')\n",
    "res_dum = dum.score(X_test, y_test)\n",
    "print(\"The accuracy of LDA Classifier: \", res_dum*100, \"%\")\n",
    "\n",
    "## SVM\n",
    "\n",
    "svm = SVC()\n",
    "start6 = time.time()\n",
    "svm.fit(X_train,y_train)\n",
    "stop6 = time.time()\n",
    "print(f'Training time of SVM : {stop6-start6}s')\n",
    "res_svm = svm.score(X_test, y_test)\n",
    "print(\"The accuracy of SVM: \", res_svm*100, \"%\")\n",
    "\n",
    "## XGBClassifier\n",
    "\n",
    "mod_XCB = xgboost.XGBClassifier()\n",
    "start7 = time.time()\n",
    "mod_XCB.fit(X_train,y_train)\n",
    "stop7 = time.time()\n",
    "print(f'Training time of XGB : {stop7-start7}s')\n",
    "result8 = mod_XCB.score(X_test, y_test)\n",
    "print('The Accuracy of XGBClassifier: ', result8*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
